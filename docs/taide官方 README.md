---
license: other
license_name: taide-l-models-community-license-agreement
license_link: https://drive.google.com/file/d/1ICTxogjS9Bc2O3K1P9ZauQYVoruT13n5/view
extra_gated_heading: 您需要先同意授權條款才能使用此模型
extra_gated_fields:
  姓名(Name): text
  生日(Date of birth): date_picker
  國家(Country): country
  所屬單位(Affiliation): text
  geo: ip_location
  按下送出表示您同意社群授權同意書與個人資料蒐集告知聲明(By clicking Submit below I accept the terms of the license and privacy policy): checkbox

extra_gated_prompt: >-
  * ### [（Llama 版次）-TAIDE 模型授權條款](https://drive.google.com/file/d/1ICTxogjS9Bc2O3K1P9ZauQYVoruT13n5/view)
  
  * ### [個人資料蒐集告知聲明(Privacy policy)](https://drive.google.com/file/d/1MfYktH3jBK61YVA1yBLruU7nZlKWFYGd/view)
  
extra_gated_button_content: 送出(Submit)
---
* [English Version](./README_en.md)

# 模型簡介
* [TAIDE計畫](https://taide.tw/index)致力於開發符合台灣語言和文化特性的生成式人工智慧對話引擎模型，同時建構可信任的人工智慧環境。結合產學研能量，推動可信任生成式人工智慧的發展，提升台灣在國際競爭中的地位，促進產業發展，避免對外國技術的依賴。
* Llama 3.1 TAIDE 系列模型以 Meta 公司釋出的 [Llama 3.1-8B](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1) 為基礎，導入台灣不同領域可用的文本與訓練素材，提高模型在正體中文回應的能力與特定任務的表現。公開釋出的模型如下:
    * [Llama-3.1-TAIDE-LX-8B-Chat](https://huggingface.co/TLLM/Llama-3.1-TAIDE-LX-8B-Chat): 以 Llama 3.1-8B 為基礎，使用正體中文資料預訓練 (continuous pretraining)，並透過指令微調(instruction tuning)強化辦公室常用任務和多輪問答對話能力，適合聊天對話或任務協助的使用情境。

# 模型參數
* 參數量: 8.5B
* 最大內容長度 (context length): 131,584
* 繁中訓練資料 token 量: 45B
* 訓練時間: 2896 H100 GPU Hours

# 特色
* 嚴格把關模型的訓練資料，提升模型生成資料的可信任性和適用性
* 針對自動摘要、寫信、寫文章、中翻英、英翻中等辦公室常用任務做加強
* 針對台灣在地文化、用語、國情等知識做加強
* 具備多輪問答對話能力
* 提升長文處理能力
* 正體中文的解碼(decoding)速度快 20%

# 應用範例
| 任務 | 使用者輸入 | 模型輸出 |
| --- | -------- | ------- |
| 寫文章 | 請以以下內容為基礎，寫一篇文章：撰寫一篇作文，題目為《一張舊照片》，內容要求為：選擇一張令你印象深刻的照片，說明令你印象深刻的原因，並描述照片中的影像及背後的故事。記錄成長的過程、與他人的情景、環境變遷和美麗的景色。 | 一張舊照片，勾起了我心中最深刻的回憶。這張照片是我的曾祖父母年輕時的合影，背景中是一段充滿回憶的的海岸線。相片中，我的曾祖母穿著一襲簡約的旗袍，手持一束玫瑰，笑容如花般燦爛。我的曾祖父則是穿著一套筆挺的西裝，神情自信，一手在曾祖母的肩上，另一手則是指著遠方的景色。<br><br>令我印象深刻的原因是，這張照片是我的家族成員在幾十年前，所拍下的。在那個年代，拍照是一件大事，需要花費不少的時間和金錢。我的家族在當時的經濟環境下，並非富裕，卻仍願意花費時間和金錢來拍下這張照片。這種精神令我深感佩服和感動。<br><br>照片中的影像，呈現了一段美好的時光。我的曾祖母的笑容，依舊清晰地印在我的腦海中。當我看著這張照片時，我可以感受到她的快樂和幸福。背景中的海岸線，當時是一段未開發的秘境，充滿著未知的危機和機會。在那個年代，我的家族在這片土地上，種植了作物、捕魚和養豬，一點一滴地努力，打造了一個穩定的家園。<br><br>照片所呈現的場景，還包括了當時的環境變遷。相片中的背景是一段海岸線，我可以看見當時的海岸線是一段未開發的秘境，到處都是雜草和石頭。隨著時間的推移，我的家族在這片土地上，進行著開墾和建設。高大的房屋、寬闊的馬路和繁榮的市集，一一出現在我的眼前。這個景象，讓我知道我的家族在這片土地上，付出了多少的努力和心血。<br><br>這張舊照片，還記錄了美麗的景色。相片中的背景，呈現了一段渾然天成的海岸風光。遼闊的天空、廣闊的大海和連綿的山脈，一一地呈現在我的眼前。我的曾祖母和曾祖父的背影，逐漸地消失在這片景色中，留下的是一個永恆的回憶。<br><br>這張舊照片，不只是一張相片，而是一段回憶的傳承。它記錄了我的家族在這片土地上，所經歷過的成長、努力和變遷。每當我看著這張相片時，我都能感受到我的家族在這片土地上，所付出的心血和努力。這張舊照片，不但是一個家族的故事，也是一個時代的見證。<br><br>在這張舊照片的背後，我還能感受到一種家族的情感。我的曾祖母和曾祖父的笑容，讓我感受到了家族的愛和關懷。相片中的景象，不但呈現了一段美好的時光，也呈現了家族的傳統和精神。每當我看著這張相片時，我都能感受到我的家族在這片土地上，所留下的痕跡和情感。<br><br>這張舊照片，不但是一張相片，也是一個回憶的催化劑。它讓我回想起了我的家族在這片土地上，所經歷過的種種故事和回憶。每當我看著這張相片時，我都能感受到我的家族在這片土地上，所留下的愛和關懷。 |
| 寫信   | 請以品牌經理的身份，給廣告公司的創意總監寫一封信，提出對於新產品廣告宣傳活動的創意建議。 | 尊敬的創意總監：<br><br>首先，感謝您在百忙之中抽出時間閱讀我的信。我代表本公司的行銷部門，對於即將推出的新產品「超能電池」感到非常興奮。我們相信，這個具有革命性突破的新產品，將為市場帶來重大的影響。為了確保新產品的成功上市，並引起目標客群的關注，我們希望在廣告宣傳方面能夠做到最好。<br>為此，我提出幾項創意建議，供您在策劃新產品廣告活動時作為參考。<br>一、創意概念：<br>我們建議以「超乎想像，力挽萬象」作為廣告活動的核心概念。這個概念要傳達出新產品「超能電池」的超高性能，以及它能夠為各種裝置充電的強大能力。在廣告中可以加入一些科幻元素，如未來世界、高科技產品等，以吸引目標客群的注意。<br>二、跨界合作：<br>為了擴大產品的影響力，我們建議與業界內其他知名品牌合作，共同推廣新產品。例如，與知名手機製造商合作，推出限量版搭載「超能電池」的手機；或與熱門電競品牌合作，讓新產品成為電競賽事的指定電源裝置。這不但能提高產品認知度，還能增加產品在目標客群中的好感度。<br>三、故事行銷：<br>人類從古至今都喜歡聽故事，故事行銷是一種能打動人心的行銷手法。我們在廣告中可以編織一個關於「超能電池」的傳奇故事，描述它如何在關鍵時刻拯救了某人或某物，展現出它的神祕與強大。這種故事行銷的方式能引起目標客群的共鳴，加深他們對產品的印象。<br>四、網路社群操作：<br>在數位時代，社群媒體已成為人們獲取資訊的重要來源。我們建議在Facebook、Instagram、YouTube等平台上，設立與新產品相關的粉絲專頁和頻道。利用網路紅人、KOL或KOC的力量，讓他們體驗並分享「超能電池」的使用心得。同時，我們也可以舉辦網路活動，如線上有獎徵答、產品試用抽獎等，吸引目標客群參與，提高產品知名度。<br>五、戶外廣告：<br>在城市街道、地鐵站、機場等高流量場所，設置大型戶外廣告牌，展示新產品「超能電池」的形象和優勢。這些醒目的廣告可以吸引過往的行人目光，對他們產生印象深刻的影響。此外，我們在廣告牌上加入AR（擴增實境）技術，讓消費者可以透過手機螢幕看到產品栩栩如生的樣子，增加互動性和趣味性。<br>六、新聞發布會：<br>為了給新產品一個隆重的登場，我們建議舉辦一場新聞發布會。在會上，我們可以邀請業界專家、媒體記者和目標客群代表等出席，向大家介紹新產品的技術規格、特點和優勢。同時，我們也可以設想一些現場體驗環節，讓嘉賓親身體驗「超能電池」的威力。<br>以上是我對新產品廣告宣傳活動的幾項創意建議。希望這些想法能對您有所幫助，並為新產品的上市帶來巨大的成功。如果您有任何疑問或需進一步討論，請隨時與我聯絡。謝謝您的關注，期待與您共創輝煌的廣告事業！<br>致敬，<br>[您的姓名]<br>[您的職銜]<br>[您的公司]  |
| 摘要   | 請將這篇文章精簡條理化:「產業創新條例第10條之2及第72條條文修正案」俗稱「台版晶片法」,針對半導體、電動車、5G等技術創新且居國際供應鏈關鍵地位公司,提供最高25%營所稅投抵優惠,企業適用要件包含當年度研發費用、研發密度達一定規模,且有效稅率達一定比率。<br>為因應經濟合作暨發展組織(OECD)國家最低稅負制調整,其中有效稅率門檻,民國112年訂為12%,113年料將提高至15%,但仍得審酌國際間最低稅負制實施情形。<br>經濟部官員表示,已和財政部協商進入最後階段,除企業研發密度訂在6%,目前已確認,企業購置先進製程的設備投資金額達100億元以上可抵減。<br>財政部官員表示,研商過程中,針對台灣產業與其在國際間類似的公司進行深入研究,在設備部分,畢竟適用產創10之2的業者是代表台灣隊打「國際盃」,投入金額不達100億元,可能也打不了。<br>至於備受關注的研發費用門檻,經濟部官員表示,歷經與財政部來回密切討論,研發費用門檻有望落在60億至70億元之間。<br>財政部官員指出,研發攸關台灣未來經濟成長動能,門檻不能「高不可攀」,起初雖設定在100億元,之所以會調降,正是盼讓企業覺得有辦法達得到門檻、進而適用租稅優惠,才有動力繼續投入研發,維持國際供應鏈關鍵地位。<br>經濟部官員表示,因廠商研發費用平均為30、40億元,其中,IC設計業者介於30億至60億元範圍,若將門檻訂在100億元,符合條件的業者較少、刺激誘因不足;此外,若符合申請門檻的業者增加,將可提高企業在台投資金額,財政部稅收也能因此獲得挹注。<br>IC設計業者近日頻頻針對產創10之2發聲,希望降低適用門檻,加上各國力拚供應鏈自主化、加碼補助半導體產業,經濟部官員表示,經濟部和財政部就產創10之2達成共識,爭取讓更多業者受惠,盼增強企業投資力道及鞏固台灣技術地位。<br>財政部官員表示,租稅獎勵的制定必須「有為有守」,並以達到獎勵設置目的為最高原則,現階段在打「國內盃」的企業仍可適用產創第10條、10之1的租稅優惠,共同壯大台灣經濟發展。<br>經濟部和財政部正就研發費用門檻做最後確認,待今明兩天預告子法之後,約有30天時間,可與業界進一步討論及調整,盼產創10之2能在6月上路。 | 2023年5月15日起的台灣「產業創新條例」第10條之2和第72條修正重點為針對半導體、電動車、5G等國際供應鏈關鍵企業提供研發密度達標且繳納一定稅率的企業最高25%的稅收抵免優惠。此「台版晶片法」旨在鼓勵技術創新。企業適用條件包括研發費用佔當年度營業額的6%以上、研發密度達到一定規模及稅率高於特定門檻。因應國際最低稅負的調整，台灣的有效稅率門檻將在今後兩年內從12%提高到15%。<br>經濟部與財政部正在就企業購置先進製程設備的投資抵減額進行最後商議，並確認了100億元的投資門檻。對於這一制度，財政部和經濟部已進行廣泛的研究，並取得了共識，希望通過降低門檻鼓勵企業加碼研發投入，維持台灣在世界市場供應鏈中的關鍵地位。<br>為進一步鼓勵研發投入，目前門檻預計將落在之前估計的60億至70億元之間。財政部關注研發對台灣未來的重要性，設定門檻不能過高，以免嚇退企業。此外，希望通過調整門檻提高企業對研發投資的動力。<br>在業界的呼籲下，經濟部和財政部已經就降低產創10之2的門檻達成了一致，期望加強企業投資能力，並鞏固台灣在科技領域的領導地位。<br>對於租稅優惠的制定，財政部強調須維持一貫性且優先用於促進產業發展，並且會在不影響財政收入的情況下進行調整。在未來，企業仍將可以舊系統的優惠條文申請稅收減免，配合新制度的實施。經濟部和財政部正在對研發費用門檻做最後的確認，預告子法發佈後與業界溝通細調，希望盡快在6月實施產創10之2。 |
| 翻譯   | 以下提供英文內容，請幫我翻譯成中文。Dongshan coffee is famous for its unique position, and the constant refinement of production methods. The flavor is admired by many caffeine afficionados. | 東山咖啡聞名遐邇，其獨樹一幟的地方在在展現，而源源不絕的改良更是令人驚嘆。眾多咖啡愛好者無不為其醇厚口感與馥郁香氣傾倒。 |

# 使用說明
* 範例程式
    * 範例程式與文件後續會釋出到github
* prompt 樣版
    * 一般問答用法
        ```python
        chat = [
            {"role": "user", "content": "{question}"},
        ]
        prompt = tokenizer.apply_chat_template(chat)
        ```
        * 將 {question} 替換成使用者的輸入
    * 加入 system prompt 的用法
        ```python
        chat = [
            {"role": "system", "content": "{sys}"},
            {"role": "user", "content": "{question}"},
        ]
        prompt = tokenizer.apply_chat_template(chat)
        ```
        * 將 {sys} 替換成指令，本模型的system prompt為：你是一個來自台灣的AI助理，你的名字是 TAIDE，樂於以台灣人的立場幫助使用者，會用正體中文回答問題。
        * 將 {question} 替換成使用者的問題
    * 多輪問答用法
        ```python
        chat = [
            {"role": "system", "content": "{sys}"},
            {"role": "user", "content": "{question1}"},
            {"role": "assistant", "content": "{model_anwer_1}"},
            {"role": "user", "content": "{question2}"},
        ]
        prompt = tokenizer.apply_chat_template(chat)
        ```
        * 將 {sys} 替換成指令，例如：你是一個來自台灣的AI助理，你的名字是 TAIDE，樂於以台灣人的立場幫助使用者，會用正體中文回答問題。
        * 將 {question1} 替換成使用者的問題1
        * 將 {model_anwer_1} 替換成模型的回答1
        * 將 {question2} 替換成使用者的問題2
    * 更多細節請參考[Llama 3.1 文件](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1)

# 訓練方法
* 軟硬體規格
    * 國網中心 H100
    * 訓練框架: PyTorch
* 資料前處理
    * 字元標準化
    * 去除重覆
    * 去除雜訊
        * 網頁資料的html tag、javascript
        * 非標準字元或亂碼
        * 字數過短的文章
        * 去除文章中的特定格式，如為排版增加的換行
    * 去除個資，如email、電話
    * 去除不當文字，如賭博、色情等
* 持續預訓練 (continuous pretraining, CP)
    * 補充大量來源可信賴的正體中文知識
    * 超參數 (hyper parameters)
        * optimizer: FusedAdam
        * learning rate: 1e-4
        * batch size: 1M tokens
        * epoch: 1
* 微調 (fine tune, FT)
    * 讓模型可針對正體中文提問回答問題
    * 超參數 (hyper parameters)
        * optimizer: FusedAdam
        * learning rate: 1e-5
        * batch size: 256K tokens
        * epoch: 5

# 訓練資料
* 持續預訓練資料(資料量約為240G)
| 資料集 | 資料描述 |
| --- | -------- |
| 訴訟資料 | 《[司法院裁判書](https://judgment.judicial.gov.tw/FJUD/default.aspx)》自2013年1月至2023年12月各級法院民事、刑事、行政訴訟資料。 |
| 中央社 | 《[中央社中文新聞](https://www.cna.com.tw/)》資料集含中央社自1993年6月至2023年06月，共30年份之每日新聞文章。內容涵蓋國內外政治、社會、財經、文教、生活等領域。 |
| ETtoday 新聞雲 | 《[ETtoday新聞雲](https://www.ettoday.net/)》資料，包含自2011年10月至 2023年12月的資料。 |
| 立法院公報 | 《[立法院公報](https://ppg.ly.gov.tw/ppg/)》包含自第8屆第1會期至第10屆第7會期之公報資料。 |
| 出版商網站書籍介紹 | 包含[三采](https://www.suncolor.com.tw/)、[Gotop](https://www.gotop.com.tw/)出版商網站上的書籍簡介。 |
| GRB 研究計畫摘要 | [GRB](https://www.grb.gov.tw/)為收錄由政府經費補助之研究計畫及其成果報告的資訊系統，此資料集主要收錄 1993年至 2023年之研究計畫摘要以及研究報告摘要，含中文及其英文對照。 |
| 學術會議論文摘要 | 收錄《[學術會議論文摘要資料庫](https://sticnet.stpi.narl.org.tw/sticloc/ttscalle?meet:)》中自1988至2009年由台灣所舉辦之學術會議論文。 |
| 光華雜誌 | 《[台灣光華雜誌](https://www.taiwan-panorama.com/)》含自1993年7月至2023年6月的文章，共30年份。內容著重於我國文化、觀光與民情等。 |
| 樂詞網 | 《[樂詞網](https://terms.naer.edu.tw/)》涵蓋文理領域約187萬則學術名詞及其譯名對照。 |
| 各部會資料 | 包含行政院「[國情簡介](https://www.ey.gov.tw/state/)」、文化部「[國家文化記憶庫](https://memory.culture.tw/)」、國發會「[檔案支援教學網](https://art.archives.gov.tw/index.aspx)」、交通部「[交通安全入口網](https://168.motc.gov.tw/)」等部會網站資料之部分資料。 |
| 今周刊 | 《[今周刊](https://www.businesstoday.com.tw/)》為一以財經為主的週刊雜誌，此資料集涵蓋2008年1月至2023年7月的文章。 |
| 教育部國語辭典、成語辭典 | 包含以下三項資料:<br>[教育部《成語典》](https://dict.idioms.moe.edu.tw/search.jsp?webMd=1&la=0)，含5,338條成語，內容包含每條成語的釋義、典故原文及其白話說明、用法說明、例句等。<br>[教育部《重編國語辭典修訂本》](https://dict.revised.moe.edu.tw/?la=0&powerMode=0)，收錄中文單字及各類辭彙，包含讀音、部首、釋義等資訊，共約165,539筆資料。<br>[教育部《國語辭典簡編本》](https://dict.concised.moe.edu.tw/?la=0&powerMode=0)，為《重編國語辭典修訂本》的簡編版本，共45,247筆資料。 |
| 科技大觀園資料 | 含《[科技大觀園網站](https://scitechvista.nat.gov.tw/)》上的科學新知以及科普文章。 |
| iKnow 科技產業資訊室 | 《[科技產業資訊室](https://iknow.stpi.narl.org.tw/)（iKnow）》提供台灣及全球的科技市場趨勢、策略分析、專利知識，及技術交易資訊，專注於科技產業的創新與發展，包含自 2008 年至 2023 年。 |
| 科學發展月刊 | 《[科學發展月刊](https://ejournal.stpi.narl.org.tw/sd)》為國科會為推廣科學教育而出版的科普刊物，含自2004年10月至2020年12月之科普文章；2021年起，以《[科技魅癮](https://www.charmingscitech.nat.gov.tw/)》季刊重新出發，提供國際關注科技議題的新知文章。 |
| 法規資料庫 | 《[法規資料庫](https://law.moj.gov.tw/)》含截自 112 年 10 月各政府部門最新發布之中央法規、行政規則、法規命令草案及地方自治法規等。 |
| 各地政府旅遊網 | 涵蓋台灣部分縣市地方政府觀光旅遊網站上之部分資料。 |
| 國教院課程綱要(十二年國教) | 含十二年國教課程綱要之總綱以及各級學校不同科目之課程綱要。 |
| 中央社譯名檔資料庫 | 《中央社譯名檔資料庫》蒐集中央社新聞業務上翻譯過的中外姓氏、人名、組織、地名等譯名對照。 |
| 童話書 | 共 20 本童話書，含湯姆歷險記、小飛俠、愛麗絲夢遊仙境、長腿叔叔等。 |
| RedPajama-Data-V2 | 從國外開放多國語言語料庫 [RedPajama-Data-v2](https://github.com/togethercomputer/RedPajama-Data) 取出英文資料 |
| MathPile-commercial | 國外開放數學語料庫 [MathPile-commercial](https://huggingface.co/datasets/GAIR/MathPile_Commercial) |
| 中文維基百科 | 《[中文維基百科](https://zh.wikipedia.org/zh-tw/%E4%B8%AD%E6%96%87%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91)》截至2023年1月所有條目的內容。 |
| github-code-clean | 為 github 開源程式碼資料集，去除unlicense的程式碼和文件。 |
* 微調資料
    * TAIDE團隊訓練Llama2系列模型來產生微調資料，產生的任務包含世界知識、創意寫作、普通常識、翻譯、摘要、程式、台灣價值等單輪或多輪對話問答，共 133K 筆。

# 模型評測
* taide-bench
    * 評測資料
        * 寫文章、寫信、摘要、英翻中、中翻英，共500題
        * 資料連結: [taide-bench](https://huggingface.co/datasets/taide/taide-bench)
    * 評測方法
        * gpt4評分
        * 評分程式: [taide-bench-eval](https://github.com/taide-taiwan/taide-bench-eval)
    * 評測分數
| 模型 | 中翻英 | 英翻中 | 摘要 | 寫文章 | 寫信 | 平均 |
| --- | ----- | ----- | ---- | ---- | ---- | --- |
| Llama-3.1-TAIDE-LX-8B-Chat | 7.50 | 8.00 | 7.33 | 8.75 | 9.05 | 8.126 |
| Llama3-TAIDE-LX-8B-Chat-Alpha1 | 7.77 | 8.28 | 8.50 | 9.61 | 8.95 | 8.620 |
| meta-llama/Llama-3.1-8B-Instruct | 6.63 | 7.76 | 7.12 | 7.71 | 7.79 | 7.402 |

* CLongEval (長文評測)
    * 評測資料
        * 涵蓋長篇故事QA、對話記憶、故事摘要、新聞堆疊標註、錯字堆疊檢測、關鍵段落檢索、表格查詢等七種任務，共 7,267 筆測試資料，並依上下文長度（約 1K～16K、16K～50K、50K～100K tokens）分為小／中／大型子集（2,605／2,653／2,005 筆）；TAIDE 以 OpenCC 轉換為繁體中文進行評測。
        * 資料連結: [CLongEval (Hugging Face)](https://huggingface.co/datasets/zexuanqiu22/CLongEval)
    * 評測方法
        * 評測指標式評分
        * 評分程式: [CLongEval](https://github.com/zexuanqiu/CLongEval)
    * 評測分數
| 模型 | 小型子集 | 中型子集 | 大型子集 | 所有資料 |
| --- | ------- | ------- | ------- | --- |
| Llama-3.1-TAIDE-LX-8B-Chat | 21.62 | 15.40 | 10.64 | 15.94 |
| Llama3-TAIDE-LX-8B-Chat-Alpha1 | 7.32 | 0.00 | 0.00 | 2.37 |
| meta-llama/Llama-3.1-8B-Instruct | 33.51 | 22.49 | 19.24 | 25.31 |

# 授權條款
* [（Llama 版次）-TAIDE 模型授權條款](https://drive.google.com/file/d/1ICTxogjS9Bc2O3K1P9ZauQYVoruT13n5/view)

# 免責聲明
* LLM 模型由於設計架構的限制，以及資料難免有偏誤，語言模型的任何回應不代表 TAIDE 立場，使用前需要額外加入安全防護機制，且回應內容也可能包含不正確的資訊，使用者請勿盡信。

# 開發團隊
* [https://taide.tw/index/teamList](https://taide.tw/index/teamList)

# 相關連結
* [TAIDE官網](https://taide.tw/index)
* [TAIDE Huggingface](https://huggingface.co/taide)
* [TAIDE Github](https://github.com/taide-taiwan)
* [Kuwa AI](https://kuwaai.org/)

# Citation
* [TAIDE官網](https://taide.tw/index)
